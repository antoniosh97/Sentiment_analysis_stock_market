{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea principal de este trabajo es determinar si se puede llegar a predecir el comportamiento de ciertas acciones en bolsa teniendo en cuenta criterios no puramente analiticos. Para ello, se llevará a cabo un analisis de sentimiento de los 25 titulares de los foros de redit mas importantes relacionados con temas bursatiles. Una vez analizados estos titulares, se introduciran las variables procesadas como features del modelo de clasificacion a entrenar. Dicho modelo intentará predecir si las acciones van a subir(label=1) o a bajar (label=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://catanacapital.com/wp-content/uploads/2019/05/Market_Emotions_Cycle.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos las librerias que vamos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos con dos datasets:\n",
    "\n",
    "- 1º Combined_News_DJIA: Los 25 titulares de reddit mencionados anteriormente\n",
    "\n",
    "- 2º upload_DJIA_table: Datos bursatiles del indice DowJones30 (open, high, close etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "redit = pd.read_csv('Combined_News_DJIA.csv')#todos los titulos de foros de reddit\n",
    "dj30 = pd.read_csv('upload_DJIA_table.csv')#open,high,low etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1989, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj30.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1989, 27)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Label', 'Top1', 'Top2', 'Top3', 'Top4', 'Top5', 'Top6', 'Top7',\n",
       "       'Top8', 'Top9', 'Top10', 'Top11', 'Top12', 'Top13', 'Top14', 'Top15',\n",
       "       'Top16', 'Top17', 'Top18', 'Top19', 'Top20', 'Top21', 'Top22', 'Top23',\n",
       "       'Top24', 'Top25'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>b\"Georgia 'downs two Russian warplanes' as cou...</td>\n",
       "      <td>b'BREAKING: Musharraf to be impeached.'</td>\n",
       "      <td>b'Russia Today: Columns of troops roll into So...</td>\n",
       "      <td>b'Russian tanks are moving towards the capital...</td>\n",
       "      <td>b\"Afghan children raped with 'impunity,' U.N. ...</td>\n",
       "      <td>b'150 Russian tanks have entered South Ossetia...</td>\n",
       "      <td>b\"Breaking: Georgia invades South Ossetia, Rus...</td>\n",
       "      <td>b\"The 'enemy combatent' trials are nothing but...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Georgia Invades South Ossetia - if Russia ge...</td>\n",
       "      <td>b'Al-Qaeda Faces Islamist Backlash'</td>\n",
       "      <td>b'Condoleezza Rice: \"The US would not act to p...</td>\n",
       "      <td>b'This is a busy day:  The European Union has ...</td>\n",
       "      <td>b\"Georgia will withdraw 1,000 soldiers from Ir...</td>\n",
       "      <td>b'Why the Pentagon Thinks Attacking Iran is a ...</td>\n",
       "      <td>b'Caucasus in crisis: Georgia invades South Os...</td>\n",
       "      <td>b'Indian shoe manufactory  - And again in a se...</td>\n",
       "      <td>b'Visitors Suffering from Mental Illnesses Ban...</td>\n",
       "      <td>b\"No Help for Mexico's Kidnapping Surge\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-08-11</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Why wont America and Nato help us? If they w...</td>\n",
       "      <td>b'Bush puts foot down on Georgian conflict'</td>\n",
       "      <td>b\"Jewish Georgian minister: Thanks to Israeli ...</td>\n",
       "      <td>b'Georgian army flees in disarray as Russians ...</td>\n",
       "      <td>b\"Olympic opening ceremony fireworks 'faked'\"</td>\n",
       "      <td>b'What were the Mossad with fraudulent New Zea...</td>\n",
       "      <td>b'Russia angered by Israeli military sale to G...</td>\n",
       "      <td>b'An American citizen living in S.Ossetia blam...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Israel and the US behind the Georgian aggres...</td>\n",
       "      <td>b'\"Do not believe TV, neither Russian nor Geor...</td>\n",
       "      <td>b'Riots are still going on in Montreal (Canada...</td>\n",
       "      <td>b'China to overtake US as largest manufacturer'</td>\n",
       "      <td>b'War in South Ossetia [PICS]'</td>\n",
       "      <td>b'Israeli Physicians Group Condemns State Tort...</td>\n",
       "      <td>b' Russia has just beaten the United States ov...</td>\n",
       "      <td>b'Perhaps *the* question about the Georgia - R...</td>\n",
       "      <td>b'Russia is so much better at war'</td>\n",
       "      <td>b\"So this is what it's come to: trading sex fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>b'Remember that adorable 9-year-old who sang a...</td>\n",
       "      <td>b\"Russia 'ends Georgia operation'\"</td>\n",
       "      <td>b'\"If we had no sexual harassment we would hav...</td>\n",
       "      <td>b\"Al-Qa'eda is losing support in Iraq because ...</td>\n",
       "      <td>b'Ceasefire in Georgia: Putin Outmaneuvers the...</td>\n",
       "      <td>b'Why Microsoft and Intel tried to kill the XO...</td>\n",
       "      <td>b'Stratfor: The Russo-Georgian War and the Bal...</td>\n",
       "      <td>b\"I'm Trying to Get a Sense of This Whole Geor...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'U.S. troops still in Georgia (did you know t...</td>\n",
       "      <td>b'Why Russias response to Georgia was right'</td>\n",
       "      <td>b'Gorbachev accuses U.S. of making a \"serious ...</td>\n",
       "      <td>b'Russia, Georgia, and NATO: Cold War Two'</td>\n",
       "      <td>b'Remember that adorable 62-year-old who led y...</td>\n",
       "      <td>b'War in Georgia: The Israeli connection'</td>\n",
       "      <td>b'All signs point to the US encouraging Georgi...</td>\n",
       "      <td>b'Christopher King argues that the US and NATO...</td>\n",
       "      <td>b'America: The New Mexico?'</td>\n",
       "      <td>b\"BBC NEWS | Asia-Pacific | Extinction 'by man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-08-13</td>\n",
       "      <td>0</td>\n",
       "      <td>b' U.S. refuses Israel weapons to attack Iran:...</td>\n",
       "      <td>b\"When the president ordered to attack Tskhinv...</td>\n",
       "      <td>b' Israel clears troops who killed Reuters cam...</td>\n",
       "      <td>b'Britain\\'s policy of being tough on drugs is...</td>\n",
       "      <td>b'Body of 14 year old found in trunk; Latest (...</td>\n",
       "      <td>b'China has moved 10 *million* quake survivors...</td>\n",
       "      <td>b\"Bush announces Operation Get All Up In Russi...</td>\n",
       "      <td>b'Russian forces sink Georgian ships '</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Elephants extinct by 2020?'</td>\n",
       "      <td>b'US humanitarian missions soon in Georgia - i...</td>\n",
       "      <td>b\"Georgia's DDOS came from US sources\"</td>\n",
       "      <td>b'Russian convoy heads into Georgia, violating...</td>\n",
       "      <td>b'Israeli defence minister: US against strike ...</td>\n",
       "      <td>b'Gorbachev: We Had No Choice'</td>\n",
       "      <td>b'Witness: Russian forces head towards Tbilisi...</td>\n",
       "      <td>b' Quarter of Russians blame U.S. for conflict...</td>\n",
       "      <td>b'Georgian president  says US military will ta...</td>\n",
       "      <td>b'2006: Nobel laureate Aleksander Solzhenitsyn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-08-14</td>\n",
       "      <td>1</td>\n",
       "      <td>b'All the experts admit that we should legalis...</td>\n",
       "      <td>b'War in South Osetia - 89 pictures made by a ...</td>\n",
       "      <td>b'Swedish wrestler Ara Abrahamian throws away ...</td>\n",
       "      <td>b'Russia exaggerated the death toll in South O...</td>\n",
       "      <td>b'Missile That Killed 9 Inside Pakistan May Ha...</td>\n",
       "      <td>b\"Rushdie Condemns Random House's Refusal to P...</td>\n",
       "      <td>b'Poland and US agree to missle defense deal. ...</td>\n",
       "      <td>b'Will the Russians conquer Tblisi? Bet on it,...</td>\n",
       "      <td>...</td>\n",
       "      <td>b'Bank analyst forecast Georgian crisis 2 days...</td>\n",
       "      <td>b\"Georgia confict could set back Russia's US r...</td>\n",
       "      <td>b'War in the Caucasus is as much the product o...</td>\n",
       "      <td>b'\"Non-media\" photos of South Ossetia/Georgia ...</td>\n",
       "      <td>b'Georgian TV reporter shot by Russian sniper ...</td>\n",
       "      <td>b'Saudi Arabia: Mother moves to block child ma...</td>\n",
       "      <td>b'Taliban wages war on humanitarian aid workers'</td>\n",
       "      <td>b'Russia: World  \"can forget about\" Georgia\\'s...</td>\n",
       "      <td>b'Darfur rebels accuse Sudan of mounting major...</td>\n",
       "      <td>b'Philippines : Peace Advocate say Muslims nee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Label                                               Top1  \\\n",
       "0  2008-08-08      0  b\"Georgia 'downs two Russian warplanes' as cou...   \n",
       "1  2008-08-11      1  b'Why wont America and Nato help us? If they w...   \n",
       "2  2008-08-12      0  b'Remember that adorable 9-year-old who sang a...   \n",
       "3  2008-08-13      0  b' U.S. refuses Israel weapons to attack Iran:...   \n",
       "4  2008-08-14      1  b'All the experts admit that we should legalis...   \n",
       "\n",
       "                                                Top2  \\\n",
       "0            b'BREAKING: Musharraf to be impeached.'   \n",
       "1        b'Bush puts foot down on Georgian conflict'   \n",
       "2                 b\"Russia 'ends Georgia operation'\"   \n",
       "3  b\"When the president ordered to attack Tskhinv...   \n",
       "4  b'War in South Osetia - 89 pictures made by a ...   \n",
       "\n",
       "                                                Top3  \\\n",
       "0  b'Russia Today: Columns of troops roll into So...   \n",
       "1  b\"Jewish Georgian minister: Thanks to Israeli ...   \n",
       "2  b'\"If we had no sexual harassment we would hav...   \n",
       "3  b' Israel clears troops who killed Reuters cam...   \n",
       "4  b'Swedish wrestler Ara Abrahamian throws away ...   \n",
       "\n",
       "                                                Top4  \\\n",
       "0  b'Russian tanks are moving towards the capital...   \n",
       "1  b'Georgian army flees in disarray as Russians ...   \n",
       "2  b\"Al-Qa'eda is losing support in Iraq because ...   \n",
       "3  b'Britain\\'s policy of being tough on drugs is...   \n",
       "4  b'Russia exaggerated the death toll in South O...   \n",
       "\n",
       "                                                Top5  \\\n",
       "0  b\"Afghan children raped with 'impunity,' U.N. ...   \n",
       "1      b\"Olympic opening ceremony fireworks 'faked'\"   \n",
       "2  b'Ceasefire in Georgia: Putin Outmaneuvers the...   \n",
       "3  b'Body of 14 year old found in trunk; Latest (...   \n",
       "4  b'Missile That Killed 9 Inside Pakistan May Ha...   \n",
       "\n",
       "                                                Top6  \\\n",
       "0  b'150 Russian tanks have entered South Ossetia...   \n",
       "1  b'What were the Mossad with fraudulent New Zea...   \n",
       "2  b'Why Microsoft and Intel tried to kill the XO...   \n",
       "3  b'China has moved 10 *million* quake survivors...   \n",
       "4  b\"Rushdie Condemns Random House's Refusal to P...   \n",
       "\n",
       "                                                Top7  \\\n",
       "0  b\"Breaking: Georgia invades South Ossetia, Rus...   \n",
       "1  b'Russia angered by Israeli military sale to G...   \n",
       "2  b'Stratfor: The Russo-Georgian War and the Bal...   \n",
       "3  b\"Bush announces Operation Get All Up In Russi...   \n",
       "4  b'Poland and US agree to missle defense deal. ...   \n",
       "\n",
       "                                                Top8  ...  \\\n",
       "0  b\"The 'enemy combatent' trials are nothing but...  ...   \n",
       "1  b'An American citizen living in S.Ossetia blam...  ...   \n",
       "2  b\"I'm Trying to Get a Sense of This Whole Geor...  ...   \n",
       "3             b'Russian forces sink Georgian ships '  ...   \n",
       "4  b'Will the Russians conquer Tblisi? Bet on it,...  ...   \n",
       "\n",
       "                                               Top16  \\\n",
       "0  b'Georgia Invades South Ossetia - if Russia ge...   \n",
       "1  b'Israel and the US behind the Georgian aggres...   \n",
       "2  b'U.S. troops still in Georgia (did you know t...   \n",
       "3                      b'Elephants extinct by 2020?'   \n",
       "4  b'Bank analyst forecast Georgian crisis 2 days...   \n",
       "\n",
       "                                               Top17  \\\n",
       "0                b'Al-Qaeda Faces Islamist Backlash'   \n",
       "1  b'\"Do not believe TV, neither Russian nor Geor...   \n",
       "2       b'Why Russias response to Georgia was right'   \n",
       "3  b'US humanitarian missions soon in Georgia - i...   \n",
       "4  b\"Georgia confict could set back Russia's US r...   \n",
       "\n",
       "                                               Top18  \\\n",
       "0  b'Condoleezza Rice: \"The US would not act to p...   \n",
       "1  b'Riots are still going on in Montreal (Canada...   \n",
       "2  b'Gorbachev accuses U.S. of making a \"serious ...   \n",
       "3             b\"Georgia's DDOS came from US sources\"   \n",
       "4  b'War in the Caucasus is as much the product o...   \n",
       "\n",
       "                                               Top19  \\\n",
       "0  b'This is a busy day:  The European Union has ...   \n",
       "1    b'China to overtake US as largest manufacturer'   \n",
       "2         b'Russia, Georgia, and NATO: Cold War Two'   \n",
       "3  b'Russian convoy heads into Georgia, violating...   \n",
       "4  b'\"Non-media\" photos of South Ossetia/Georgia ...   \n",
       "\n",
       "                                               Top20  \\\n",
       "0  b\"Georgia will withdraw 1,000 soldiers from Ir...   \n",
       "1                     b'War in South Ossetia [PICS]'   \n",
       "2  b'Remember that adorable 62-year-old who led y...   \n",
       "3  b'Israeli defence minister: US against strike ...   \n",
       "4  b'Georgian TV reporter shot by Russian sniper ...   \n",
       "\n",
       "                                               Top21  \\\n",
       "0  b'Why the Pentagon Thinks Attacking Iran is a ...   \n",
       "1  b'Israeli Physicians Group Condemns State Tort...   \n",
       "2          b'War in Georgia: The Israeli connection'   \n",
       "3                     b'Gorbachev: We Had No Choice'   \n",
       "4  b'Saudi Arabia: Mother moves to block child ma...   \n",
       "\n",
       "                                               Top22  \\\n",
       "0  b'Caucasus in crisis: Georgia invades South Os...   \n",
       "1  b' Russia has just beaten the United States ov...   \n",
       "2  b'All signs point to the US encouraging Georgi...   \n",
       "3  b'Witness: Russian forces head towards Tbilisi...   \n",
       "4   b'Taliban wages war on humanitarian aid workers'   \n",
       "\n",
       "                                               Top23  \\\n",
       "0  b'Indian shoe manufactory  - And again in a se...   \n",
       "1  b'Perhaps *the* question about the Georgia - R...   \n",
       "2  b'Christopher King argues that the US and NATO...   \n",
       "3  b' Quarter of Russians blame U.S. for conflict...   \n",
       "4  b'Russia: World  \"can forget about\" Georgia\\'s...   \n",
       "\n",
       "                                               Top24  \\\n",
       "0  b'Visitors Suffering from Mental Illnesses Ban...   \n",
       "1                 b'Russia is so much better at war'   \n",
       "2                        b'America: The New Mexico?'   \n",
       "3  b'Georgian president  says US military will ta...   \n",
       "4  b'Darfur rebels accuse Sudan of mounting major...   \n",
       "\n",
       "                                               Top25  \n",
       "0           b\"No Help for Mexico's Kidnapping Surge\"  \n",
       "1  b\"So this is what it's come to: trading sex fo...  \n",
       "2  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...  \n",
       "3  b'2006: Nobel laureate Aleksander Solzhenitsyn...  \n",
       "4  b'Philippines : Peace Advocate say Muslims nee...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>17924.240234</td>\n",
       "      <td>18002.380859</td>\n",
       "      <td>17916.910156</td>\n",
       "      <td>17949.369141</td>\n",
       "      <td>82160000</td>\n",
       "      <td>17949.369141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>17712.759766</td>\n",
       "      <td>17930.609375</td>\n",
       "      <td>17711.800781</td>\n",
       "      <td>17929.990234</td>\n",
       "      <td>133030000</td>\n",
       "      <td>17929.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17704.509766</td>\n",
       "      <td>17456.019531</td>\n",
       "      <td>17694.679688</td>\n",
       "      <td>106380000</td>\n",
       "      <td>17694.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>17190.509766</td>\n",
       "      <td>17409.720703</td>\n",
       "      <td>112190000</td>\n",
       "      <td>17409.720703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17355.210938</td>\n",
       "      <td>17063.080078</td>\n",
       "      <td>17140.240234</td>\n",
       "      <td>138740000</td>\n",
       "      <td>17140.240234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Open          High           Low         Close  \\\n",
       "0  2016-07-01  17924.240234  18002.380859  17916.910156  17949.369141   \n",
       "1  2016-06-30  17712.759766  17930.609375  17711.800781  17929.990234   \n",
       "2  2016-06-29  17456.019531  17704.509766  17456.019531  17694.679688   \n",
       "3  2016-06-28  17190.509766  17409.720703  17190.509766  17409.720703   \n",
       "4  2016-06-27  17355.210938  17355.210938  17063.080078  17140.240234   \n",
       "\n",
       "      Volume     Adj Close  \n",
       "0   82160000  17949.369141  \n",
       "1  133030000  17929.990234  \n",
       "2  106380000  17694.679688  \n",
       "3  112190000  17409.720703  \n",
       "4  138740000  17140.240234  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj30.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer dataset lo conforman los titulares de las 25 noticias mas votadas de los foreros de redit diariamente. Hay un total de 1989 filas, lo que equivale, aproximadamente, a 8 años de historico. En este dataset está también la variable label. Esta etiqueta nos indica si el precio del indice del dowjones para ese dia subio o se mantuvo (=1) o bajó (=0).\n",
    "\n",
    "El segundo dataset nos muestra los valores bursatiles del indice dowjones 30: precio de apertura(open), precio de cierre(close), volumen (volumne), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date     object\n",
       "Label     int64\n",
       "Top1     object\n",
       "Top2     object\n",
       "Top3     object\n",
       "Top4     object\n",
       "Top5     object\n",
       "Top6     object\n",
       "Top7     object\n",
       "Top8     object\n",
       "Top9     object\n",
       "Top10    object\n",
       "Top11    object\n",
       "Top12    object\n",
       "Top13    object\n",
       "Top14    object\n",
       "Top15    object\n",
       "Top16    object\n",
       "Top17    object\n",
       "Top18    object\n",
       "Top19    object\n",
       "Top20    object\n",
       "Top21    object\n",
       "Top22    object\n",
       "Top23    object\n",
       "Top24    object\n",
       "Top25    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redit.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date     0\n",
       "Label    0\n",
       "Top1     0\n",
       "Top2     0\n",
       "Top3     0\n",
       "Top4     0\n",
       "Top5     0\n",
       "Top6     0\n",
       "Top7     0\n",
       "Top8     0\n",
       "Top9     0\n",
       "Top10    0\n",
       "Top11    0\n",
       "Top12    0\n",
       "Top13    0\n",
       "Top14    0\n",
       "Top15    0\n",
       "Top16    0\n",
       "Top17    0\n",
       "Top18    0\n",
       "Top19    0\n",
       "Top20    0\n",
       "Top21    0\n",
       "Top22    0\n",
       "Top23    1\n",
       "Top24    3\n",
       "Top25    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redit.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muy pocos nan. Sustituyo esos valores por  ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "redit = redit.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que los datos siguen estando muy sucios. La persona que hizo scraping no cogio solo las palabras y quedan caracteres especiales que podrian fastidiar el analisis. Sobretodo tengo que limpiar tres tipos de errores:\n",
    "\n",
    "- b'\n",
    "- b\"\n",
    "-  \\ '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"Georgia \\'downs two Russian warplanes\\' as countries move to brink of war\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redit['Top1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos expresiones regulares para limpiar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Georgia downs two Russian warplanes as countries move to brink of war\"'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacements = [\n",
    "    (\"b[(')]\", ''),\n",
    "    ('b[(\")]', ''),\n",
    "    ('(\\')', '')    \n",
    "]\n",
    "\n",
    "clean=redit['Top1'][0]\n",
    "for old, new in replacements:\n",
    "    clean = re.sub(old, new, clean)\n",
    "    \n",
    "clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "incorporamos dicha correccion y pasamos todo a minusculas en el dataset redit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in range(2,redit.shape[1]):\n",
    "    for i in range(redit.shape[0]):\n",
    "        for old, new in replacements:\n",
    "            try:\n",
    "                redit.iloc[i,column] = re.sub(old, new, redit.iloc[i,column].lower())\n",
    "            except:\n",
    "                continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>georgia downs two russian warplanes as countri...</td>\n",
       "      <td>breaking: musharraf to be impeached.</td>\n",
       "      <td>russia today: columns of troops roll into sout...</td>\n",
       "      <td>russian tanks are moving towards the capital o...</td>\n",
       "      <td>afghan children raped with impunity, u.n. offi...</td>\n",
       "      <td>150 russian tanks have entered south ossetia w...</td>\n",
       "      <td>breaking: georgia invades south ossetia, russi...</td>\n",
       "      <td>the enemy combatent trials are nothing but a s...</td>\n",
       "      <td>...</td>\n",
       "      <td>georgia invades south ossetia - if russia gets...</td>\n",
       "      <td>al-qaeda faces islamist backlash</td>\n",
       "      <td>condoleezza rice: \"the us would not act to pre...</td>\n",
       "      <td>this is a busy day:  the european union has ap...</td>\n",
       "      <td>georgia will withdraw 1,000 soldiers from iraq...</td>\n",
       "      <td>why the pentagon thinks attacking iran is a ba...</td>\n",
       "      <td>caucasus in crisis: georgia invades south ossetia</td>\n",
       "      <td>indian shoe manufactory  - and again in a seri...</td>\n",
       "      <td>visitors suffering from mental illnesses banne...</td>\n",
       "      <td>no help for mexicos kidnapping surge\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-08-11</td>\n",
       "      <td>1</td>\n",
       "      <td>why wont america and nato help us? if they won...</td>\n",
       "      <td>bush puts foot down on georgian conflict</td>\n",
       "      <td>jewish georgian minister: thanks to israeli tr...</td>\n",
       "      <td>georgian army flees in disarray as russians ad...</td>\n",
       "      <td>olympic opening ceremony fireworks faked\"</td>\n",
       "      <td>what were the mossad with fraudulent new zeala...</td>\n",
       "      <td>russia angered by israeli military sale to geo...</td>\n",
       "      <td>an american citizen living in s.ossetia blames...</td>\n",
       "      <td>...</td>\n",
       "      <td>israel and the us behind the georgian aggression?</td>\n",
       "      <td>\"do not believe tv, neither russian nor georgi...</td>\n",
       "      <td>riots are still going on in montreal (canada) ...</td>\n",
       "      <td>china to overtake us as largest manufacturer</td>\n",
       "      <td>war in south ossetia [pics]</td>\n",
       "      <td>israeli physicians group condemns state torture</td>\n",
       "      <td>russia has just beaten the united states over...</td>\n",
       "      <td>perhaps *the* question about the georgia - rus...</td>\n",
       "      <td>russia is so much better at war</td>\n",
       "      <td>so this is what its come to: trading sex for f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>remember that adorable 9-year-old who sang at ...</td>\n",
       "      <td>russia ends georgia operation\"</td>\n",
       "      <td>\"if we had no sexual harassment we would have ...</td>\n",
       "      <td>al-qaeda is losing support in iraq because of ...</td>\n",
       "      <td>ceasefire in georgia: putin outmaneuvers the west</td>\n",
       "      <td>why microsoft and intel tried to kill the xo $...</td>\n",
       "      <td>stratfor: the russo-georgian war and the balan...</td>\n",
       "      <td>im trying to get a sense of this whole georgia...</td>\n",
       "      <td>...</td>\n",
       "      <td>u.s. troops still in georgia (did you know the...</td>\n",
       "      <td>why russias response to georgia was right</td>\n",
       "      <td>gorbachev accuses u.s. of making a \"serious bl...</td>\n",
       "      <td>russia, georgia, and nato: cold war two</td>\n",
       "      <td>remember that adorable 62-year-old who led you...</td>\n",
       "      <td>war in georgia: the israeli connection</td>\n",
       "      <td>all signs point to the us encouraging georgia ...</td>\n",
       "      <td>christopher king argues that the us and nato a...</td>\n",
       "      <td>america: the new mexico?</td>\n",
       "      <td>bbc news | asia-pacific | extinction by man no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-08-13</td>\n",
       "      <td>0</td>\n",
       "      <td>u.s. refuses israel weapons to attack iran: r...</td>\n",
       "      <td>when the president ordered to attack tskhinval...</td>\n",
       "      <td>israel clears troops who killed reuters camer...</td>\n",
       "      <td>britain\\s policy of being tough on drugs is \"p...</td>\n",
       "      <td>body of 14 year old found in trunk; latest (ra...</td>\n",
       "      <td>china has moved 10 *million* quake survivors i...</td>\n",
       "      <td>bush announces operation get all up in russias...</td>\n",
       "      <td>russian forces sink georgian ships</td>\n",
       "      <td>...</td>\n",
       "      <td>elephants extinct by 2020?</td>\n",
       "      <td>us humanitarian missions soon in georgia - if ...</td>\n",
       "      <td>georgias ddos came from us sources\"</td>\n",
       "      <td>russian convoy heads into georgia, violating t...</td>\n",
       "      <td>israeli defence minister: us against strike on...</td>\n",
       "      <td>gorbachev: we had no choice</td>\n",
       "      <td>witness: russian forces head towards tbilisi i...</td>\n",
       "      <td>quarter of russians blame u.s. for conflict: ...</td>\n",
       "      <td>georgian president  says us military will take...</td>\n",
       "      <td>2006: nobel laureate aleksander solzhenitsyn a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-08-14</td>\n",
       "      <td>1</td>\n",
       "      <td>all the experts admit that we should legalise ...</td>\n",
       "      <td>war in south osetia - 89 pictures made by a ru...</td>\n",
       "      <td>swedish wrestler ara abrahamian throws away me...</td>\n",
       "      <td>russia exaggerated the death toll in south oss...</td>\n",
       "      <td>missile that killed 9 inside pakistan may have...</td>\n",
       "      <td>rushdie condemns random houses refusal to publ...</td>\n",
       "      <td>poland and us agree to missle defense deal. in...</td>\n",
       "      <td>will the russians conquer tblisi? bet on it, n...</td>\n",
       "      <td>...</td>\n",
       "      <td>bank analyst forecast georgian crisis 2 days e...</td>\n",
       "      <td>georgia confict could set back russias us rela...</td>\n",
       "      <td>war in the caucasus is as much the product of ...</td>\n",
       "      <td>\"non-media\" photos of south ossetia/georgia co...</td>\n",
       "      <td>georgian tv reporter shot by russian sniper du...</td>\n",
       "      <td>saudi arabia: mother moves to block child marr...</td>\n",
       "      <td>taliban wages war on humanitarian aid workers</td>\n",
       "      <td>russia: world  \"can forget about\" georgia\\s te...</td>\n",
       "      <td>darfur rebels accuse sudan of mounting major a...</td>\n",
       "      <td>philippines : peace advocate say muslims need ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Label                                               Top1  \\\n",
       "0  2008-08-08      0  georgia downs two russian warplanes as countri...   \n",
       "1  2008-08-11      1  why wont america and nato help us? if they won...   \n",
       "2  2008-08-12      0  remember that adorable 9-year-old who sang at ...   \n",
       "3  2008-08-13      0   u.s. refuses israel weapons to attack iran: r...   \n",
       "4  2008-08-14      1  all the experts admit that we should legalise ...   \n",
       "\n",
       "                                                Top2  \\\n",
       "0               breaking: musharraf to be impeached.   \n",
       "1           bush puts foot down on georgian conflict   \n",
       "2                     russia ends georgia operation\"   \n",
       "3  when the president ordered to attack tskhinval...   \n",
       "4  war in south osetia - 89 pictures made by a ru...   \n",
       "\n",
       "                                                Top3  \\\n",
       "0  russia today: columns of troops roll into sout...   \n",
       "1  jewish georgian minister: thanks to israeli tr...   \n",
       "2  \"if we had no sexual harassment we would have ...   \n",
       "3   israel clears troops who killed reuters camer...   \n",
       "4  swedish wrestler ara abrahamian throws away me...   \n",
       "\n",
       "                                                Top4  \\\n",
       "0  russian tanks are moving towards the capital o...   \n",
       "1  georgian army flees in disarray as russians ad...   \n",
       "2  al-qaeda is losing support in iraq because of ...   \n",
       "3  britain\\s policy of being tough on drugs is \"p...   \n",
       "4  russia exaggerated the death toll in south oss...   \n",
       "\n",
       "                                                Top5  \\\n",
       "0  afghan children raped with impunity, u.n. offi...   \n",
       "1          olympic opening ceremony fireworks faked\"   \n",
       "2  ceasefire in georgia: putin outmaneuvers the west   \n",
       "3  body of 14 year old found in trunk; latest (ra...   \n",
       "4  missile that killed 9 inside pakistan may have...   \n",
       "\n",
       "                                                Top6  \\\n",
       "0  150 russian tanks have entered south ossetia w...   \n",
       "1  what were the mossad with fraudulent new zeala...   \n",
       "2  why microsoft and intel tried to kill the xo $...   \n",
       "3  china has moved 10 *million* quake survivors i...   \n",
       "4  rushdie condemns random houses refusal to publ...   \n",
       "\n",
       "                                                Top7  \\\n",
       "0  breaking: georgia invades south ossetia, russi...   \n",
       "1  russia angered by israeli military sale to geo...   \n",
       "2  stratfor: the russo-georgian war and the balan...   \n",
       "3  bush announces operation get all up in russias...   \n",
       "4  poland and us agree to missle defense deal. in...   \n",
       "\n",
       "                                                Top8  ...  \\\n",
       "0  the enemy combatent trials are nothing but a s...  ...   \n",
       "1  an american citizen living in s.ossetia blames...  ...   \n",
       "2  im trying to get a sense of this whole georgia...  ...   \n",
       "3                russian forces sink georgian ships   ...   \n",
       "4  will the russians conquer tblisi? bet on it, n...  ...   \n",
       "\n",
       "                                               Top16  \\\n",
       "0  georgia invades south ossetia - if russia gets...   \n",
       "1  israel and the us behind the georgian aggression?   \n",
       "2  u.s. troops still in georgia (did you know the...   \n",
       "3                         elephants extinct by 2020?   \n",
       "4  bank analyst forecast georgian crisis 2 days e...   \n",
       "\n",
       "                                               Top17  \\\n",
       "0                   al-qaeda faces islamist backlash   \n",
       "1  \"do not believe tv, neither russian nor georgi...   \n",
       "2          why russias response to georgia was right   \n",
       "3  us humanitarian missions soon in georgia - if ...   \n",
       "4  georgia confict could set back russias us rela...   \n",
       "\n",
       "                                               Top18  \\\n",
       "0  condoleezza rice: \"the us would not act to pre...   \n",
       "1  riots are still going on in montreal (canada) ...   \n",
       "2  gorbachev accuses u.s. of making a \"serious bl...   \n",
       "3                georgias ddos came from us sources\"   \n",
       "4  war in the caucasus is as much the product of ...   \n",
       "\n",
       "                                               Top19  \\\n",
       "0  this is a busy day:  the european union has ap...   \n",
       "1       china to overtake us as largest manufacturer   \n",
       "2            russia, georgia, and nato: cold war two   \n",
       "3  russian convoy heads into georgia, violating t...   \n",
       "4  \"non-media\" photos of south ossetia/georgia co...   \n",
       "\n",
       "                                               Top20  \\\n",
       "0  georgia will withdraw 1,000 soldiers from iraq...   \n",
       "1                        war in south ossetia [pics]   \n",
       "2  remember that adorable 62-year-old who led you...   \n",
       "3  israeli defence minister: us against strike on...   \n",
       "4  georgian tv reporter shot by russian sniper du...   \n",
       "\n",
       "                                               Top21  \\\n",
       "0  why the pentagon thinks attacking iran is a ba...   \n",
       "1    israeli physicians group condemns state torture   \n",
       "2             war in georgia: the israeli connection   \n",
       "3                        gorbachev: we had no choice   \n",
       "4  saudi arabia: mother moves to block child marr...   \n",
       "\n",
       "                                               Top22  \\\n",
       "0  caucasus in crisis: georgia invades south ossetia   \n",
       "1   russia has just beaten the united states over...   \n",
       "2  all signs point to the us encouraging georgia ...   \n",
       "3  witness: russian forces head towards tbilisi i...   \n",
       "4      taliban wages war on humanitarian aid workers   \n",
       "\n",
       "                                               Top23  \\\n",
       "0  indian shoe manufactory  - and again in a seri...   \n",
       "1  perhaps *the* question about the georgia - rus...   \n",
       "2  christopher king argues that the us and nato a...   \n",
       "3   quarter of russians blame u.s. for conflict: ...   \n",
       "4  russia: world  \"can forget about\" georgia\\s te...   \n",
       "\n",
       "                                               Top24  \\\n",
       "0  visitors suffering from mental illnesses banne...   \n",
       "1                    russia is so much better at war   \n",
       "2                           america: the new mexico?   \n",
       "3  georgian president  says us military will take...   \n",
       "4  darfur rebels accuse sudan of mounting major a...   \n",
       "\n",
       "                                               Top25  \n",
       "0              no help for mexicos kidnapping surge\"  \n",
       "1  so this is what its come to: trading sex for f...  \n",
       "2  bbc news | asia-pacific | extinction by man no...  \n",
       "3  2006: nobel laureate aleksander solzhenitsyn a...  \n",
       "4  philippines : peace advocate say muslims need ...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empiezo el analisis de sentimiento con **vader** y **TextBlob**, determinando positividad, negatividad, neutralidad, polaridad y subjetividad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob devuelve la polaridad y subjetividad de una frse. La polaridad se encuentra entre [-1,1], -1 define un sentimiento negativo y 1 define un sentimiento positivo. Las palabras de negación invierten la polaridad. TextBlob tiene etiquetas semánticas que ayudan con el análisis detallado. Por ejemplo: emoticonos, signos de exclamación, emojis, etc. La subjetividad se encuentra entre [0,1], esta cuantifica la cantidad de opiniones personales e información fáctica contenida en el texto. La subjetividad más alta significa que el texto contiene opiniones personales en lugar de información fáctica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que hacemos primero es quedarnos con las variables que queremos analizar (los titulares), creando el dataset sentiment_vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer() #instancio el objeto sentimentAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_vader=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_vader=redit.columns\n",
    "columnas_vader=columnas_vader[2:]\n",
    "sentiment_vader=redit[columnas_vader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1989, 25)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_vader.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_neg=pd.DataFrame()\n",
    "vader_pos=pd.DataFrame()\n",
    "vader_neu=pd.DataFrame()\n",
    "vader_tot=pd.DataFrame()\n",
    "vader_pol=pd.DataFrame()\n",
    "vader_sub=pd.DataFrame()\n",
    "\n",
    "for column in range(sentiment_vader.shape[1]):\n",
    "    neg=[]\n",
    "    pos=[]\n",
    "    neu=[]\n",
    "    tot=[]\n",
    "    pol=[]\n",
    "    sub=[]\n",
    "    \n",
    "    for i in range(sentiment_vader.shape[0]):\n",
    "        try:\n",
    "            neg.append(sid.polarity_scores(sentiment_vader.iloc[i,column])['neg'])\n",
    "            pos.append(sid.polarity_scores(sentiment_vader.iloc[i,column])['pos'])\n",
    "            neu.append(sid.polarity_scores(sentiment_vader.iloc[i,column])['neu'])\n",
    "            tot.append(sid.polarity_scores(sentiment_vader.iloc[i,column])['compound'])\n",
    "            pol.append(TextBlob(sentiment_vader.iloc[i,column]).sentiment.polarity)\n",
    "            sub.append(TextBlob(sentiment_vader.iloc[i,column]).sentiment.subjectivity)\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    vader_neg[columnas_vader[column]+'neg']=neg\n",
    "    vader_pos[columnas_vader[column]+'pos']=pos\n",
    "    vader_neu[columnas_vader[column]+'neu']=neu\n",
    "    vader_tot[columnas_vader[column]+'tot']=tot\n",
    "    vader_pol[columnas_vader[column]+'pol']=pol\n",
    "    vader_sub[columnas_vader[column]+'pol']=sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top1neg</th>\n",
       "      <th>Top2neg</th>\n",
       "      <th>Top3neg</th>\n",
       "      <th>Top4neg</th>\n",
       "      <th>Top5neg</th>\n",
       "      <th>Top6neg</th>\n",
       "      <th>Top7neg</th>\n",
       "      <th>Top8neg</th>\n",
       "      <th>Top9neg</th>\n",
       "      <th>Top10neg</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16neg</th>\n",
       "      <th>Top17neg</th>\n",
       "      <th>Top18neg</th>\n",
       "      <th>Top19neg</th>\n",
       "      <th>Top20neg</th>\n",
       "      <th>Top21neg</th>\n",
       "      <th>Top22neg</th>\n",
       "      <th>Top23neg</th>\n",
       "      <th>Top24neg</th>\n",
       "      <th>Top25neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.262</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Top1neg  Top2neg  Top3neg  Top4neg  Top5neg  Top6neg  Top7neg  Top8neg  \\\n",
       "0    0.262    0.000    0.172    0.247    0.424    0.000    0.149    0.107   \n",
       "1    0.000    0.277    0.000    0.240    0.000    0.225    0.320    0.134   \n",
       "2    0.169    0.000    0.497    0.254    0.000    0.320    0.328    0.140   \n",
       "3    0.500    0.249    0.417    0.073    0.163    0.000    0.000    0.000   \n",
       "4    0.000    0.302    0.000    0.412    0.281    0.346    0.000    0.231   \n",
       "\n",
       "   Top9neg  Top10neg  ...  Top16neg  Top17neg  Top18neg  Top19neg  Top20neg  \\\n",
       "0    0.246     0.328  ...     0.196     0.000     0.078     0.092     0.112   \n",
       "1    0.310     0.324  ...     0.239     0.178     0.391     0.000     0.494   \n",
       "2    0.000     0.485  ...     0.000     0.000     0.198     0.394     0.289   \n",
       "3    0.085     0.000  ...     0.000     0.000     0.000     0.368     0.169   \n",
       "4    0.412     0.255  ...     0.406     0.000     0.302     0.315     0.000   \n",
       "\n",
       "   Top21neg  Top22neg  Top23neg  Top24neg  Top25neg  \n",
       "0     0.351     0.406     0.140     0.650     0.247  \n",
       "1     0.643     0.169     0.247     0.323     0.000  \n",
       "2     0.438     0.000     0.074     0.000     0.000  \n",
       "3     0.355     0.000     0.439     0.177     0.194  \n",
       "4     0.293     0.394     0.181     0.573     0.000  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_neg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma me quedan 6 dataframes diferentes. En cada uno se analiza la positividad, negtividad, neutralidad,'compound', polaridad y subjetividad de los textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que quiero conseguir es la media ponderada de la positividad, negatividad o neutralidad. Para ello tengo que definir unos pesos. Los pesos van de 25 a 1 (25 es el número de titulares por día) en funcion del numero en el top de importancia. De esta forma el top 1 tiene peso 25 y el top 25 tiene peso 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos=list(range(1,26))\n",
    "pesos=pesos[::-1]\n",
    "pesos[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pondero los pesos y transformo los dataframes anteriores en uno solo llamado resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negatividad</th>\n",
       "      <th>positividad</th>\n",
       "      <th>neutralidad</th>\n",
       "      <th>total</th>\n",
       "      <th>polaridad</th>\n",
       "      <th>subjetividad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.190154</td>\n",
       "      <td>0.021237</td>\n",
       "      <td>0.788625</td>\n",
       "      <td>-0.368780</td>\n",
       "      <td>-0.040314</td>\n",
       "      <td>0.163685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.165105</td>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.743665</td>\n",
       "      <td>-0.094385</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>0.202921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.195689</td>\n",
       "      <td>0.050437</td>\n",
       "      <td>0.753865</td>\n",
       "      <td>-0.318394</td>\n",
       "      <td>-0.062091</td>\n",
       "      <td>0.377127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.150452</td>\n",
       "      <td>0.041649</td>\n",
       "      <td>0.807951</td>\n",
       "      <td>-0.197096</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.176371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.181422</td>\n",
       "      <td>0.094557</td>\n",
       "      <td>0.723985</td>\n",
       "      <td>-0.205634</td>\n",
       "      <td>0.047797</td>\n",
       "      <td>0.319615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negatividad  positividad  neutralidad     total  polaridad  subjetividad\n",
       "0     0.190154     0.021237     0.788625 -0.368780  -0.040314      0.163685\n",
       "1     0.165105     0.091188     0.743665 -0.094385   0.034991      0.202921\n",
       "2     0.195689     0.050437     0.753865 -0.318394  -0.062091      0.377127\n",
       "3     0.150452     0.041649     0.807951 -0.197096   0.001742      0.176371\n",
       "4     0.181422     0.094557     0.723985 -0.205634   0.047797      0.319615"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados=pd.DataFrame()\n",
    "resultados['negatividad'] = vader_neg.mul(pesos, axis=1).sum(axis=1)/sum(pesos)\n",
    "resultados['positividad'] = vader_pos.mul(pesos, axis=1).sum(axis=1)/sum(pesos)\n",
    "resultados['neutralidad'] = vader_neu.mul(pesos, axis=1).sum(axis=1)/sum(pesos)\n",
    "resultados['total'] = vader_tot.mul(pesos, axis=1).sum(axis=1)/sum(pesos)\n",
    "resultados['polaridad'] = vader_pol.mul(pesos, axis=1).sum(axis=1)/sum(pesos)\n",
    "resultados['subjetividad'] = vader_sub.mul(pesos, axis=1).sum(axis=1)/sum(pesos)\n",
    "resultados.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añado a este dataframe la fecha y la label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>negatividad</th>\n",
       "      <th>positividad</th>\n",
       "      <th>neutralidad</th>\n",
       "      <th>total</th>\n",
       "      <th>polaridad</th>\n",
       "      <th>subjetividad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.190154</td>\n",
       "      <td>0.021237</td>\n",
       "      <td>0.788625</td>\n",
       "      <td>-0.368780</td>\n",
       "      <td>-0.040314</td>\n",
       "      <td>0.163685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-08-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.165105</td>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.743665</td>\n",
       "      <td>-0.094385</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>0.202921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.195689</td>\n",
       "      <td>0.050437</td>\n",
       "      <td>0.753865</td>\n",
       "      <td>-0.318394</td>\n",
       "      <td>-0.062091</td>\n",
       "      <td>0.377127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-08-13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150452</td>\n",
       "      <td>0.041649</td>\n",
       "      <td>0.807951</td>\n",
       "      <td>-0.197096</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.176371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-08-14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181422</td>\n",
       "      <td>0.094557</td>\n",
       "      <td>0.723985</td>\n",
       "      <td>-0.205634</td>\n",
       "      <td>0.047797</td>\n",
       "      <td>0.319615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Label  negatividad  positividad  neutralidad     total  \\\n",
       "0  2008-08-08      0     0.190154     0.021237     0.788625 -0.368780   \n",
       "1  2008-08-11      1     0.165105     0.091188     0.743665 -0.094385   \n",
       "2  2008-08-12      0     0.195689     0.050437     0.753865 -0.318394   \n",
       "3  2008-08-13      0     0.150452     0.041649     0.807951 -0.197096   \n",
       "4  2008-08-14      1     0.181422     0.094557     0.723985 -0.205634   \n",
       "\n",
       "   polaridad  subjetividad  \n",
       "0  -0.040314      0.163685  \n",
       "1   0.034991      0.202921  \n",
       "2  -0.062091      0.377127  \n",
       "3   0.001742      0.176371  \n",
       "4   0.047797      0.319615  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final=redit.iloc[:,0:2]\n",
    "df_final = df_final.join(resultados,how='outer')\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiero que mi modelo predictivo tambien cuente con features como el precio de la accion o el volumen, por eso uno este dataset con el del DJ30. Como luego compararé que modelo sale mejor, el que incluye al precio y a variables 'objetivas' o al que solo incluye variables 'subjetivas', defino este modelo como df_final_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>negatividad</th>\n",
       "      <th>positividad</th>\n",
       "      <th>neutralidad</th>\n",
       "      <th>total</th>\n",
       "      <th>polaridad</th>\n",
       "      <th>subjetividad</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>2008-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.190154</td>\n",
       "      <td>0.021237</td>\n",
       "      <td>0.788625</td>\n",
       "      <td>-0.368780</td>\n",
       "      <td>-0.040314</td>\n",
       "      <td>0.163685</td>\n",
       "      <td>11432.089844</td>\n",
       "      <td>11759.959961</td>\n",
       "      <td>11388.040039</td>\n",
       "      <td>11734.320312</td>\n",
       "      <td>212830000</td>\n",
       "      <td>11734.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>2008-08-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.165105</td>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.743665</td>\n",
       "      <td>-0.094385</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>0.202921</td>\n",
       "      <td>11729.669922</td>\n",
       "      <td>11867.110352</td>\n",
       "      <td>11675.530273</td>\n",
       "      <td>11782.349609</td>\n",
       "      <td>183190000</td>\n",
       "      <td>11782.349609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>2008-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.195689</td>\n",
       "      <td>0.050437</td>\n",
       "      <td>0.753865</td>\n",
       "      <td>-0.318394</td>\n",
       "      <td>-0.062091</td>\n",
       "      <td>0.377127</td>\n",
       "      <td>11781.700195</td>\n",
       "      <td>11782.349609</td>\n",
       "      <td>11601.519531</td>\n",
       "      <td>11642.469727</td>\n",
       "      <td>173590000</td>\n",
       "      <td>11642.469727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>2008-08-13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150452</td>\n",
       "      <td>0.041649</td>\n",
       "      <td>0.807951</td>\n",
       "      <td>-0.197096</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.176371</td>\n",
       "      <td>11632.809570</td>\n",
       "      <td>11633.780273</td>\n",
       "      <td>11453.339844</td>\n",
       "      <td>11532.959961</td>\n",
       "      <td>182550000</td>\n",
       "      <td>11532.959961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>2008-08-14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181422</td>\n",
       "      <td>0.094557</td>\n",
       "      <td>0.723985</td>\n",
       "      <td>-0.205634</td>\n",
       "      <td>0.047797</td>\n",
       "      <td>0.319615</td>\n",
       "      <td>11532.070312</td>\n",
       "      <td>11718.280273</td>\n",
       "      <td>11450.889648</td>\n",
       "      <td>11615.929688</td>\n",
       "      <td>159790000</td>\n",
       "      <td>11615.929688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Label  negatividad  positividad  neutralidad     total  \\\n",
       "1988  2008-08-08      0     0.190154     0.021237     0.788625 -0.368780   \n",
       "1987  2008-08-11      1     0.165105     0.091188     0.743665 -0.094385   \n",
       "1986  2008-08-12      0     0.195689     0.050437     0.753865 -0.318394   \n",
       "1985  2008-08-13      0     0.150452     0.041649     0.807951 -0.197096   \n",
       "1984  2008-08-14      1     0.181422     0.094557     0.723985 -0.205634   \n",
       "\n",
       "      polaridad  subjetividad          Open          High           Low  \\\n",
       "1988  -0.040314      0.163685  11432.089844  11759.959961  11388.040039   \n",
       "1987   0.034991      0.202921  11729.669922  11867.110352  11675.530273   \n",
       "1986  -0.062091      0.377127  11781.700195  11782.349609  11601.519531   \n",
       "1985   0.001742      0.176371  11632.809570  11633.780273  11453.339844   \n",
       "1984   0.047797      0.319615  11532.070312  11718.280273  11450.889648   \n",
       "\n",
       "             Close     Volume     Adj Close  \n",
       "1988  11734.320312  212830000  11734.320312  \n",
       "1987  11782.349609  183190000  11782.349609  \n",
       "1986  11642.469727  173590000  11642.469727  \n",
       "1985  11532.959961  182550000  11532.959961  \n",
       "1984  11615.929688  159790000  11615.929688  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_2=df_final.merge(dj30, how='inner', on = 'Date', left_index=True)\n",
    "df_final_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date            False\n",
       "Label           False\n",
       "negatividad     False\n",
       "positividad     False\n",
       "neutralidad     False\n",
       "total           False\n",
       "polaridad       False\n",
       "subjetividad    False\n",
       "Open            False\n",
       "High            False\n",
       "Low             False\n",
       "Close           False\n",
       "Volume          False\n",
       "Adj Close       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_2.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo predictivo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comienzo el analisis predictivo con un clasificador simple para tener un resultado base a mejorar. Para ello uso un kvecinos. Se empieza el analisis con el dataframe que **no** incluye informacion bursatil, solo analisis de sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_final.drop(['Label','Date'],axis=1)\n",
    "y=df_final['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=10)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5053272450532724"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos ahora con un modelo un poco mas robusto. En este caso el LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5144596651445966"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos una busqueda de metaparametros intentando mejorar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttest's auc: 0.538879\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.536311\n",
      "Early stopping, best iteration is:\n",
      "[78]\ttest's auc: 0.542627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.544607\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttest's auc: 0.550624\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.530003\n",
      "Early stopping, best iteration is:\n",
      "[102]\ttest's auc: 0.531369\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttest's auc: 0.537471\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttest's auc: 0.545458\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.534238\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttest's auc: 0.536647\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.547162\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttest's auc: 0.54922\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttest's auc: 0.534504\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.551167\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttest's auc: 0.55885\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttest's auc: 0.565354\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttest's auc: 0.51699\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttest's auc: 0.529891\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.535632\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttest's auc: 0.541715\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttest's auc: 0.537986\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttest's auc: 0.529872\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttest's auc: 0.539623\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.538047\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttest's auc: 0.544574\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttest's auc: 0.551317\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttest's auc: 0.545968\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttest's auc: 0.528585\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.540021\n",
      "[200]\ttest's auc: 0.547541\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttest's auc: 0.548636\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttest's auc: 0.551438\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.531716\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttest's auc: 0.536432\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.54339\n",
      "[200]\ttest's auc: 0.551204\n",
      "[300]\ttest's auc: 0.557512\n",
      "Early stopping, best iteration is:\n",
      "[292]\ttest's auc: 0.558476\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttest's auc: 0.556263\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttest's auc: 0.535492\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttest's auc: 0.528347\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttest's auc: 0.544317\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttest's auc: 0.545458\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttest's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttest's auc: 0.541668\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttest's auc: 0.534579\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttest's auc: 0.532277\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.540316\n",
      "Early stopping, best iteration is:\n",
      "[152]\ttest's auc: 0.547396\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttest's auc: 0.545987\n",
      "Early stopping, best iteration is:\n",
      "[150]\ttest's auc: 0.550596\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttest's auc: 0.53537\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttest's auc: 0.537448\n",
      "Mejor score alcanzado: 0.5285285285285285 con los parametros: {'max_depth': 11, 'min_child_weight': 119, 'num_leaves': 860} \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [(X_test,y_test)],\n",
    "            'eval_names': ['test'],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "import lightgbm \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 1500), \n",
    "             'max_depth': sp_randint(3,20),\n",
    "             'min_child_weight': sp_randint(10, 250)}\n",
    "\n",
    "n_HP_points_to_test = 30\n",
    "\n",
    "clf = LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4,n_estimators=2000)\n",
    "\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)\n",
    "\n",
    "gs.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "print('Mejor score alcanzado: {} con los parametros: {} '.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5372907153729072"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgb = LGBMClassifier(n_estimators=200,\n",
    "                         num_leaves=860,\n",
    "                         max_depth=11,\n",
    "                         min_child_weight=119)\n",
    "\n",
    "model_lgb.fit(X_train, y_train)\n",
    "model_lgb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pese a haber hecho una busqueda de metaparametros y haber utilizado un modelo mas robusto como el el LGBM seguimos teniendo un score bastante bajo(~0.5). Para mejorar el modelo vamos a utilizar tambien datos bursatiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_final_2.drop(['Label','Date','Adj Close'],axis=1)#quito el cierre ajustado tambien. me quedo solo con el cierre 'close'\n",
    "y=df_final['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negatividad</th>\n",
       "      <th>positividad</th>\n",
       "      <th>neutralidad</th>\n",
       "      <th>total</th>\n",
       "      <th>polaridad</th>\n",
       "      <th>subjetividad</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>0.125828</td>\n",
       "      <td>0.057083</td>\n",
       "      <td>0.817086</td>\n",
       "      <td>-0.183346</td>\n",
       "      <td>0.034436</td>\n",
       "      <td>0.230929</td>\n",
       "      <td>10037.849609</td>\n",
       "      <td>10161.570312</td>\n",
       "      <td>9976.709961</td>\n",
       "      <td>10144.190430</td>\n",
       "      <td>194470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.145483</td>\n",
       "      <td>0.025868</td>\n",
       "      <td>0.828662</td>\n",
       "      <td>-0.320976</td>\n",
       "      <td>-0.010479</td>\n",
       "      <td>0.220251</td>\n",
       "      <td>16712.699219</td>\n",
       "      <td>16795.980469</td>\n",
       "      <td>16623.910156</td>\n",
       "      <td>16639.970703</td>\n",
       "      <td>98480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.092677</td>\n",
       "      <td>0.042286</td>\n",
       "      <td>0.865055</td>\n",
       "      <td>-0.121154</td>\n",
       "      <td>-0.000759</td>\n",
       "      <td>0.225032</td>\n",
       "      <td>18052.320312</td>\n",
       "      <td>18107.570312</td>\n",
       "      <td>17974.810547</td>\n",
       "      <td>17977.039062</td>\n",
       "      <td>120090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>0.113326</td>\n",
       "      <td>0.071837</td>\n",
       "      <td>0.814963</td>\n",
       "      <td>-0.075727</td>\n",
       "      <td>0.031778</td>\n",
       "      <td>0.220058</td>\n",
       "      <td>8637.650391</td>\n",
       "      <td>9026.410156</td>\n",
       "      <td>8637.490234</td>\n",
       "      <td>8934.179688</td>\n",
       "      <td>358970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.151966</td>\n",
       "      <td>0.055449</td>\n",
       "      <td>0.792538</td>\n",
       "      <td>-0.129773</td>\n",
       "      <td>0.029822</td>\n",
       "      <td>0.196389</td>\n",
       "      <td>17716.269531</td>\n",
       "      <td>17759.509766</td>\n",
       "      <td>17579.269531</td>\n",
       "      <td>17678.230469</td>\n",
       "      <td>117740000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      negatividad  positividad  neutralidad     total  polaridad  \\\n",
       "1608     0.125828     0.057083     0.817086 -0.183346   0.034436   \n",
       "88       0.145483     0.025868     0.828662 -0.320976  -0.010479   \n",
       "309      0.092677     0.042286     0.865055 -0.121154  -0.000759   \n",
       "1904     0.113326     0.071837     0.814963 -0.075727   0.031778   \n",
       "320      0.151966     0.055449     0.792538 -0.129773   0.029822   \n",
       "\n",
       "      subjetividad          Open          High           Low         Close  \\\n",
       "1608      0.230929  10037.849609  10161.570312   9976.709961  10144.190430   \n",
       "88        0.220251  16712.699219  16795.980469  16623.910156  16639.970703   \n",
       "309       0.225032  18052.320312  18107.570312  17974.810547  17977.039062   \n",
       "1904      0.220058   8637.650391   9026.410156   8637.490234   8934.179688   \n",
       "320       0.196389  17716.269531  17759.509766  17579.269531  17678.230469   \n",
       "\n",
       "         Volume  \n",
       "1608  194470000  \n",
       "88     98480000  \n",
       "309   120090000  \n",
       "1904  358970000  \n",
       "320   117740000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a intentar el mismo problema de clasificacion pero con el dataset modificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4657534246575342"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred= classifier.predict(X_test)\n",
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El score sigue saliendo bajo, probamos con otro modelo simple de clasificacion como es el de discriminacion lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9360730593607306"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred= model.predict(X_test)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93       296\n",
      "           1       0.91      0.98      0.94       361\n",
      "\n",
      "    accuracy                           0.94       657\n",
      "   macro avg       0.94      0.93      0.93       657\n",
      "weighted avg       0.94      0.94      0.94       657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo parece adaptarse muy bien al dataset y da un acuracy alto(0.93), además no tiene hiperparametros que tunear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation del modelo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model,X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95728643, 0.91959799, 0.90954774, 0.92462312, 0.94710327])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9316317101882208"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(scores,scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos ahora con una red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamos los datos, dejando la mu=0 y la desviacion =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train= sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential#inicializa la red\n",
    "from keras.layers import Dense,Dropout,BatchNormalization#agrega capas a la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el objeto clasificador(de tipo secuencial) y definimos la arquitectura de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "12/12 [==============================] - 1s 14ms/step - loss: 0.7655 - accuracy: 0.5232 - val_loss: 0.6915 - val_accuracy: 0.5300\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7509 - accuracy: 0.5283 - val_loss: 0.6928 - val_accuracy: 0.4700\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6958 - accuracy: 0.5535 - val_loss: 0.6925 - val_accuracy: 0.5400\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7037 - accuracy: 0.5729 - val_loss: 0.6921 - val_accuracy: 0.5200\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6822 - accuracy: 0.5713 - val_loss: 0.6915 - val_accuracy: 0.5050\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6800 - accuracy: 0.5924 - val_loss: 0.6926 - val_accuracy: 0.4850\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6819 - accuracy: 0.5735 - val_loss: 0.6918 - val_accuracy: 0.5150\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.6644 - accuracy: 0.6095 - val_loss: 0.6920 - val_accuracy: 0.5350\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6635 - accuracy: 0.5910 - val_loss: 0.6910 - val_accuracy: 0.5300\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6896 - accuracy: 0.5813 - val_loss: 0.6914 - val_accuracy: 0.5500\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6637 - accuracy: 0.6209 - val_loss: 0.6922 - val_accuracy: 0.5550\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6815 - accuracy: 0.5750 - val_loss: 0.6924 - val_accuracy: 0.5350\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6466 - accuracy: 0.6289 - val_loss: 0.6928 - val_accuracy: 0.5350\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6397 - accuracy: 0.6285 - val_loss: 0.6910 - val_accuracy: 0.5300\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6656 - accuracy: 0.6115 - val_loss: 0.6912 - val_accuracy: 0.5200\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6641 - accuracy: 0.5995 - val_loss: 0.6927 - val_accuracy: 0.5100\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6740 - accuracy: 0.6073 - val_loss: 0.6924 - val_accuracy: 0.5150\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6449 - accuracy: 0.6288 - val_loss: 0.6917 - val_accuracy: 0.5550\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6347 - accuracy: 0.6425 - val_loss: 0.6916 - val_accuracy: 0.5100\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6630 - accuracy: 0.6115 - val_loss: 0.6934 - val_accuracy: 0.5100\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6564 - accuracy: 0.6216 - val_loss: 0.6944 - val_accuracy: 0.4850\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6481 - accuracy: 0.6079 - val_loss: 0.6910 - val_accuracy: 0.5550\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6508 - accuracy: 0.6141 - val_loss: 0.6900 - val_accuracy: 0.5650\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6422 - accuracy: 0.6292 - val_loss: 0.6922 - val_accuracy: 0.4850\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6349 - accuracy: 0.6522 - val_loss: 0.6924 - val_accuracy: 0.5100\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6560 - accuracy: 0.6059 - val_loss: 0.6914 - val_accuracy: 0.5450\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6350 - accuracy: 0.6338 - val_loss: 0.6929 - val_accuracy: 0.5300\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6301 - accuracy: 0.6416 - val_loss: 0.6942 - val_accuracy: 0.5350\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6283 - accuracy: 0.6426 - val_loss: 0.6923 - val_accuracy: 0.5400\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6417 - accuracy: 0.6333 - val_loss: 0.6957 - val_accuracy: 0.5050\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6302 - accuracy: 0.6447 - val_loss: 0.6929 - val_accuracy: 0.5200\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6282 - accuracy: 0.6360 - val_loss: 0.6945 - val_accuracy: 0.5050\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6334 - accuracy: 0.6273 - val_loss: 0.6937 - val_accuracy: 0.4800\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6284 - accuracy: 0.6353 - val_loss: 0.6930 - val_accuracy: 0.4900\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6129 - accuracy: 0.6602 - val_loss: 0.6860 - val_accuracy: 0.5400\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6170 - accuracy: 0.6689 - val_loss: 0.6917 - val_accuracy: 0.5200\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6319 - accuracy: 0.6140 - val_loss: 0.6922 - val_accuracy: 0.5400\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6026 - accuracy: 0.6641 - val_loss: 0.6880 - val_accuracy: 0.5350\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6041 - accuracy: 0.6713 - val_loss: 0.6874 - val_accuracy: 0.5450\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6102 - accuracy: 0.6646 - val_loss: 0.6877 - val_accuracy: 0.5450\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6046 - accuracy: 0.6669 - val_loss: 0.6851 - val_accuracy: 0.5350\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6078 - accuracy: 0.6730 - val_loss: 0.6879 - val_accuracy: 0.5800\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6007 - accuracy: 0.6911 - val_loss: 0.6944 - val_accuracy: 0.5800\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.6075 - accuracy: 0.6693 - val_loss: 0.6909 - val_accuracy: 0.5650\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6187 - accuracy: 0.6571 - val_loss: 0.6827 - val_accuracy: 0.5650\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6169 - accuracy: 0.6748 - val_loss: 0.6784 - val_accuracy: 0.5950\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5675 - accuracy: 0.7265 - val_loss: 0.6813 - val_accuracy: 0.6000\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5776 - accuracy: 0.7145 - val_loss: 0.6773 - val_accuracy: 0.5750\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5565 - accuracy: 0.7226 - val_loss: 0.6764 - val_accuracy: 0.5850\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6057 - accuracy: 0.6774 - val_loss: 0.6688 - val_accuracy: 0.6350\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5668 - accuracy: 0.7255 - val_loss: 0.6661 - val_accuracy: 0.6450\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5779 - accuracy: 0.7135 - val_loss: 0.6743 - val_accuracy: 0.5700\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5782 - accuracy: 0.7041 - val_loss: 0.6572 - val_accuracy: 0.6350\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5661 - accuracy: 0.7191 - val_loss: 0.6423 - val_accuracy: 0.6150\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5953 - accuracy: 0.6785 - val_loss: 0.6254 - val_accuracy: 0.6350\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5644 - accuracy: 0.7184 - val_loss: 0.6363 - val_accuracy: 0.6050\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5481 - accuracy: 0.7197 - val_loss: 0.6452 - val_accuracy: 0.6100\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5532 - accuracy: 0.7027 - val_loss: 0.6394 - val_accuracy: 0.6350\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5230 - accuracy: 0.7589 - val_loss: 0.6759 - val_accuracy: 0.5500\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5496 - accuracy: 0.7038 - val_loss: 0.6362 - val_accuracy: 0.6550\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5690 - accuracy: 0.7028 - val_loss: 0.6383 - val_accuracy: 0.6100\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4956 - accuracy: 0.7742 - val_loss: 0.6188 - val_accuracy: 0.6300\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4829 - accuracy: 0.7903 - val_loss: 0.5829 - val_accuracy: 0.7050\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5119 - accuracy: 0.7531 - val_loss: 0.6131 - val_accuracy: 0.6750\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4985 - accuracy: 0.7518 - val_loss: 0.5777 - val_accuracy: 0.7050\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4825 - accuracy: 0.7849 - val_loss: 0.5703 - val_accuracy: 0.6950\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4997 - accuracy: 0.7353 - val_loss: 0.6236 - val_accuracy: 0.6600\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4979 - accuracy: 0.7600 - val_loss: 0.5765 - val_accuracy: 0.7100\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4774 - accuracy: 0.7841 - val_loss: 0.5622 - val_accuracy: 0.7200\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4838 - accuracy: 0.7666 - val_loss: 0.5440 - val_accuracy: 0.7250\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4790 - accuracy: 0.7651 - val_loss: 0.6339 - val_accuracy: 0.6700\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5190 - accuracy: 0.7417 - val_loss: 0.5671 - val_accuracy: 0.6650\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5020 - accuracy: 0.7713 - val_loss: 0.5512 - val_accuracy: 0.7050\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4482 - accuracy: 0.7945 - val_loss: 0.5457 - val_accuracy: 0.7100\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4776 - accuracy: 0.7574 - val_loss: 0.5319 - val_accuracy: 0.7650\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4875 - accuracy: 0.7768 - val_loss: 0.4767 - val_accuracy: 0.8050\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4272 - accuracy: 0.8189 - val_loss: 0.4941 - val_accuracy: 0.7650\n",
      "Epoch 78/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4180 - accuracy: 0.8309 - val_loss: 0.5399 - val_accuracy: 0.7150\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4589 - accuracy: 0.7677 - val_loss: 0.5085 - val_accuracy: 0.7550\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4345 - accuracy: 0.7991 - val_loss: 0.4626 - val_accuracy: 0.7850\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4050 - accuracy: 0.8085 - val_loss: 0.4618 - val_accuracy: 0.7700\n",
      "Epoch 82/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4151 - accuracy: 0.8074 - val_loss: 0.4965 - val_accuracy: 0.7650\n",
      "Epoch 83/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4411 - accuracy: 0.7971 - val_loss: 0.4574 - val_accuracy: 0.7950\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4011 - accuracy: 0.8222 - val_loss: 0.4821 - val_accuracy: 0.7600\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3845 - accuracy: 0.8317 - val_loss: 0.4373 - val_accuracy: 0.8200\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3948 - accuracy: 0.8101 - val_loss: 0.4211 - val_accuracy: 0.8300\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3700 - accuracy: 0.8544 - val_loss: 0.4817 - val_accuracy: 0.7500\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3705 - accuracy: 0.8232 - val_loss: 0.3959 - val_accuracy: 0.8500\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3381 - accuracy: 0.8559 - val_loss: 0.5573 - val_accuracy: 0.7050\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4422 - accuracy: 0.7931 - val_loss: 0.4592 - val_accuracy: 0.7750\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.4008 - accuracy: 0.8301 - val_loss: 0.4550 - val_accuracy: 0.7600\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3642 - accuracy: 0.8254 - val_loss: 0.3956 - val_accuracy: 0.8150\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3936 - accuracy: 0.8288 - val_loss: 0.4475 - val_accuracy: 0.7750\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3521 - accuracy: 0.8548 - val_loss: 0.3768 - val_accuracy: 0.8200\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8653 - val_loss: 0.4123 - val_accuracy: 0.7950\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3294 - accuracy: 0.8549 - val_loss: 0.3494 - val_accuracy: 0.8550\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3398 - accuracy: 0.8468 - val_loss: 0.3611 - val_accuracy: 0.8200\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3262 - accuracy: 0.8551 - val_loss: 0.3921 - val_accuracy: 0.8150\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3429 - accuracy: 0.8468 - val_loss: 0.3812 - val_accuracy: 0.8300\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8480 - val_loss: 0.3420 - val_accuracy: 0.8300\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3234 - accuracy: 0.8545 - val_loss: 0.2939 - val_accuracy: 0.8750\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2880 - accuracy: 0.8743 - val_loss: 0.3007 - val_accuracy: 0.8850\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.2828 - accuracy: 0.8801 - val_loss: 0.3269 - val_accuracy: 0.8450\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3120 - accuracy: 0.8611 - val_loss: 0.3379 - val_accuracy: 0.8550\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3048 - accuracy: 0.8622 - val_loss: 0.3528 - val_accuracy: 0.8250\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3572 - accuracy: 0.8320 - val_loss: 0.3845 - val_accuracy: 0.8200\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4001 - accuracy: 0.8250 - val_loss: 0.3875 - val_accuracy: 0.8100\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3472 - accuracy: 0.8433 - val_loss: 0.3849 - val_accuracy: 0.8250\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3329 - accuracy: 0.8452 - val_loss: 0.4578 - val_accuracy: 0.7600\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3159 - accuracy: 0.8444 - val_loss: 0.3973 - val_accuracy: 0.8000\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2900 - accuracy: 0.8774 - val_loss: 0.3347 - val_accuracy: 0.8600\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2829 - accuracy: 0.8734 - val_loss: 0.3472 - val_accuracy: 0.8550\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2930 - accuracy: 0.8767 - val_loss: 0.3049 - val_accuracy: 0.8600\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8423 - val_loss: 0.4819 - val_accuracy: 0.7500\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3802 - accuracy: 0.8199 - val_loss: 0.3772 - val_accuracy: 0.8150\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2876 - accuracy: 0.8606 - val_loss: 0.3483 - val_accuracy: 0.8300\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2721 - accuracy: 0.8826 - val_loss: 0.3191 - val_accuracy: 0.8650\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3026 - accuracy: 0.8596 - val_loss: 0.3356 - val_accuracy: 0.8700\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3021 - accuracy: 0.8487 - val_loss: 0.3375 - val_accuracy: 0.8650\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3276 - accuracy: 0.8481 - val_loss: 0.4006 - val_accuracy: 0.8150\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3265 - accuracy: 0.8508 - val_loss: 0.3517 - val_accuracy: 0.8450\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3275 - accuracy: 0.8465 - val_loss: 0.3472 - val_accuracy: 0.8450\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3021 - accuracy: 0.8671 - val_loss: 0.3291 - val_accuracy: 0.8600\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.2739 - accuracy: 0.8797 - val_loss: 0.3800 - val_accuracy: 0.8250\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3294 - accuracy: 0.8481 - val_loss: 0.3588 - val_accuracy: 0.8350\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2549 - accuracy: 0.8902 - val_loss: 0.3079 - val_accuracy: 0.8600\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2475 - accuracy: 0.8873 - val_loss: 0.3246 - val_accuracy: 0.8550\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2692 - accuracy: 0.8782 - val_loss: 0.3134 - val_accuracy: 0.8550\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2738 - accuracy: 0.8674 - val_loss: 0.2801 - val_accuracy: 0.8700\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2744 - accuracy: 0.8771 - val_loss: 0.3176 - val_accuracy: 0.8500\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2556 - accuracy: 0.8900 - val_loss: 0.3233 - val_accuracy: 0.8600\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2327 - accuracy: 0.9059 - val_loss: 0.2945 - val_accuracy: 0.8550\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2641 - accuracy: 0.8884 - val_loss: 0.2739 - val_accuracy: 0.8850\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2280 - accuracy: 0.9074 - val_loss: 0.2799 - val_accuracy: 0.8750\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2690 - accuracy: 0.8744 - val_loss: 0.3187 - val_accuracy: 0.8700\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2799 - accuracy: 0.8802 - val_loss: 0.2936 - val_accuracy: 0.8500\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2662 - accuracy: 0.8844 - val_loss: 0.3242 - val_accuracy: 0.8500\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.2920 - accuracy: 0.8701 - val_loss: 0.4605 - val_accuracy: 0.7800\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3242 - accuracy: 0.8339 - val_loss: 0.2576 - val_accuracy: 0.9000\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2212 - accuracy: 0.9035 - val_loss: 0.2674 - val_accuracy: 0.8850\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2615 - accuracy: 0.8835 - val_loss: 0.2575 - val_accuracy: 0.8850\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2211 - accuracy: 0.8957 - val_loss: 0.2489 - val_accuracy: 0.8800\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2543 - accuracy: 0.8827 - val_loss: 0.2507 - val_accuracy: 0.8900\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2210 - accuracy: 0.9111 - val_loss: 0.2691 - val_accuracy: 0.8950\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2300 - accuracy: 0.8958 - val_loss: 0.3895 - val_accuracy: 0.8100\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2304 - accuracy: 0.8943 - val_loss: 0.3443 - val_accuracy: 0.8550\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2308 - accuracy: 0.9132 - val_loss: 0.2986 - val_accuracy: 0.8750\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2235 - accuracy: 0.9017 - val_loss: 0.2652 - val_accuracy: 0.9000\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2012 - accuracy: 0.9095 - val_loss: 0.3602 - val_accuracy: 0.8450\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.2924 - accuracy: 0.8758 - val_loss: 0.4166 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "clasificador= Sequential()\n",
    "\n",
    "clasificador.add(Dense(units=100, kernel_initializer ='uniform', activation = 'relu', input_dim=X_train.shape[1]))\n",
    "clasificador.add(Dropout(0.2))\n",
    "clasificador.add(BatchNormalization())\n",
    "clasificador.add(Dense(units=50, activation = 'relu'))\n",
    "clasificador.add(Dropout(0.2))\n",
    "clasificador.add(Dense(units=1, activation = 'sigmoid'))\n",
    "\n",
    "clasificador.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history=clasificador.fit(X_train, y_train, batch_size=100, epochs=150, validation_split=0.15,callbacks=[tf.keras.callbacks.EarlyStopping(patience=40, mode='min', monitor= 'val_loss')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red neuronal tiene la siguiente arquitectura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 100)               1200      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,701\n",
      "Trainable params: 6,501\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clasificador.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar que no estamos haciendo overfitting del modelo pintamos en una grafica la evolucion del error en validacion y en entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clasificador.predict(X_test)\n",
    "y_pred = (y_pred>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos el score y la matriz de confusion para ver que tal los resultados obtenidos con la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mc= confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[107, 189],\n",
       "       [ 98, 263]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.563165905631659"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red neuronal, pese a ser un modelo mucho mas robusto que el de discriminacion lineal, no consigue dar resultados tan buenos, quedando una accuracy final de 0.86."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez visto los resultados se pueden llegar a ciertas conclusiones.\n",
    "\n",
    "- No es tan sencillo predecir la evolución de los precios de una accion o indice solo por analisis de sentimiento, es necesario añadir otra informacion.\n",
    "\n",
    "- Por otro lado, si que se puede usar esa informacion para enriquecer el modelo y se puede llegar a resultados buenos\n",
    "\n",
    "- Modelos simples como el de LinearDiscriminantAnalysis parecen dar mejores resultados que modelos mas robustos como una red neuronal o el LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosas a mejorar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se dejan ciertas opciones de mejora a implementar a futuro:\n",
    "    \n",
    "- Se pueden añadir nuevas features en el modelo como por ejemplo indicadores de analisis tecnico.\n",
    "\n",
    "- No se ha tenido en cuenta la temporalidad de la serie. Se podria tener en cuenta el precio o la positividad en dias anteriores con algoritmos que si tienen 'memoria' como las redes LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
